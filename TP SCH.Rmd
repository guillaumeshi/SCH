---
title: "TP Séries chronologiques"
author: "Laurent LIN & Guillaume SHI"
date: "24/02/2019"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
require(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=65), tidy=TRUE)

rm(list = ls())

# setwd("/Users/guillaumeshi/Desktop/OMA/SCH/")
setwd("/Users/LL/Documents/Centrale/GTA/OMA/SCH/DM/SCH")
library("forecast")
library("strucchange")
library("data.table")
library(lmtest)
```

# Partie 1 : Estimation du modèle

## Question 1

*Calculer le earning yield de l'indice du S&P500. Tracer un nuage de points liant le earning yield au taux sans risque. L'ajustement linéaire est-il justifié ? Utiliser la commande "abline" pour tracer cet ajustement.*

L'earning yield est défini comme étant le rapport entre l'earning et le prix d'une action. \


A première vue, on pourrait dégager une tendance linéaire avec une droite passant par 0.06 pour un taux à 2% et par 0.035 pour un taux à 6%, soit l'équation linéaire pour la régression : $y = -0.00625x + 0.0675$.
```{r}
# data <- fread("/Users/guillaumeshi/Desktop/OMA/SCH/0219_td_centrale.csv", header=TRUE)
data <- fread("0219_td_centrale.csv", header=TRUE)
data$yield <- data$earnings / data$price

plot(data$rates, data$yield, xlab="Taux", ylab="Yield",
     main="Yield en fonction du taux")
abline(0.0675, -0.00625, col="red", lwd=2)
```


## Question 2

*Rappeler le principe de la méthode des moindres carrés ordinaires. Rappeler les hypothèses sous-jacente de l'estimateur des MCO.*\


On considère la régression linéaire du vecteur des variables d'intérêt $\mathbf{y}$ en fonction du vecteur des variables explicatives $\mathbf{X}$ : il s'agit de trouver le vecteur $\boldsymbol{\gamma}$ tel que $\mathbf{y} = \mathbf{X}^T\boldsymbol{\gamma} + \boldsymbol{\epsilon}$ où $\boldsymbol{\epsilon}$ représente un bruit blanc. Les hypothèses sous-jacentes sont qu'il existe une relation linéaire entre $\mathbf{y}$ et $\mathbf{X}$, $\mathbf{X}$ doit être de rang maximum (non-colinéarité des variables explicatives),  $\mathbb{E}(\boldsymbol{\epsilon}) = \boldsymbol{0}$ (indépendance des erreurs) et $\mathbb{E}(\boldsymbol{\epsilon}\boldsymbol{\epsilon}^T) = \sigma I_n$ avec $\sigma$ constante (homoscédasticité). \


La méthode des moindres carrés ordinaires (MCO) consiste à trouver le vecteur $\boldsymbol{\gamma}$ par minimisation de la quantité suivante : $\Delta = (\mathbf{y} - \mathbf{X}^T\boldsymbol{\gamma})^2$. Ce problème de minimisation se résout en dérivant la quantité $\Delta$ par rapport à chacune des composantes de $\boldsymbol{\gamma}$ et en écrivant que la quantité obtenue est nulle : $$\forall 1\leq i \leq n, \frac{\partial \Delta}{\partial \boldsymbol{\gamma}_i} = 0$$

## Question 3

*Calculer les expressions des coefficients des estimateurs des moindres carrés et montrer que la variance de beta tend vers 0 quand T tend vers l'infini.*\

On garde les notations de la question précédente. Ici, on souhaite calibrer $\boldsymbol{\gamma}  = ( \alpha \ \ \beta)^T$ afin d'avoir une relation du type $\mathbf{y} = \frac{E_t}{P_t} = \mathbf{X}^T\boldsymbol{\gamma}$ si $\mathbf{X} = (1 \ \ r_t)^T$ désigne le vecteur des variables explicatives. Etant en présence de variables aléatoires, la quantité à minimiser est $$\mathbb{E}((\frac{E_t}{P_t} - (\alpha + \beta r_t))^2) = \mathbb{E}(\frac{E_t}{P_t}) - 2\mathbb{E}(\frac{E_t}{P_t}(\alpha + \beta r_t)) + \mathbb{E}((\alpha + \beta r_t)^2) = \Delta$$ 


- En dérivant par rapport à la première composante de $\boldsymbol{\gamma}$, c'est-à-dire $\alpha$, on trouve $$-2\mathbb{E}(\frac{E_t}{P_t}) + 2\mathbb{E}(\alpha + \beta r_t) = 0$$ soit $\alpha = \mathbb{E}(\frac{E_t}{P_t}) - \beta \mathbb{E}(r_t)$, ou encore $$\widehat{\alpha} = \overline{(\frac{E}{P})} - \widehat{\beta} \overline{r}$$ en prenant la moyenne empirique des earning yields $\overline{(\frac{E}{P})}$ comme estimateur de $\mathbb{E}(\frac{E_t}{P_t})$ et $\overline{r}$ pour $\mathbb{E}(r_t)$. \


- La dérivation par rapport à la seconde composante, $\beta$, donne $$-2 \mathbb{E}(\frac{E_t}{P_t}r_t) + 2\mathbb{E}(r_t(\alpha + \beta r_t)) = 0 = - \mathbb{E}(\frac{E_t}{P_t}r_t) + \alpha \mathbb{E}(r_t) + \beta \mathbb{E}(r_t^2)$$ soit encore en substituant à $\alpha$ sa valeur en fonction de $\beta$ trouvée précédemment :

$$- \mathbb{E}(\frac{E_t}{P_t}r_t) + \mathbb{E}(r_t)(\mathbb{E}(\frac{E_t}{P_t}) - \beta \mathbb{E}(r_t)) + \beta \mathbb{E}(r_t^2) = 0$$ d'où $$-\mathbb{E}(\frac{E_t}{P_t}r_t) + \mathbb{E}(\frac{E_t}{P_t})\mathbb{E}(r_t) - \beta \mathbb{E}(r_t)^2 + \beta \mathbb{E}(r_t^2) = 0 = -Cov(\frac{E_t}{P_t}, r_t) + \beta Var(r_t)$$ d'où l'on tire ainsi $$\widehat{\beta} = \frac{Cov(\frac{E}{P}, r)}{\sigma_r^2}$$

Montrons que $\lim_{T \rightarrow \infty} \mathrm{Var}(\hat{\beta})= 0$

- Pour calculer $\widehat{\beta}$, il faut utiliser les estimateurs empiriques des fonctions de la moyenne ($\widehat{x} = \frac{1}{T+1}\sum_{t=0}^{T} x_i$), la variance et de la covariance, ce qui donne : $\widehat{\beta} = \frac{\sum_{t=0}^{T} (r_t - \bar{r})(\frac{E_t}{P_t} - \overline{(\frac{E_t}{P_t})})}{\sum_{t=0}^{T} (r_t - \bar{r})^2}$.

- En injectant l'équation (2) décrivant le modèle vérifié par les données, on obtient :
$\mathrm{Var}(\widehat{\beta}) = \mathrm{Var}(\frac{\sum_{t=0}^{T} (r_t - \bar{r})(\alpha + \beta r_t + \epsilon_t - \overline{(\frac{E_t}{P_t})})}{\sum_{t=0}^{T} (r_t - \bar{r})^2})$

- On remarque qu'au numérateur, les $\epsilon_t$ sont les seules variables aléatoires, le reste peut être supprimé de la variance, on obtient donc : $\mathrm{Var}(\widehat{\beta}) = \mathrm{Var}( \frac{\sum_{t=0}^{T} (r_t - \bar{r})\epsilon_t}{\sum_{t=0}^{T} (r_t - \bar{r})^2} )$

- Par hypothèse, les $\epsilon_t$ sont i.i.d. donc on obtient finalement :
$$\mathrm{Var}(\widehat{\beta}) 
= \sum_{t=0}^{T} \mathrm{Var}(\frac{ (r_t - \hat{r})\epsilon_t}{\sum_{t=0}^{T} (r_t - \bar{r})^2}) 
= \frac{\sum_{t=0}^{T} (r_t - \bar{r})^2 \mathrm{Var}(\epsilon_t)}{(\sum_{t=0}^{T} (r_t - \bar{r})^2)^2}
= \sigma_{\epsilon}^2 \frac{\sum_{t=0}^{T} (r_t - \bar{r})^2}{(\sum_{t=0}^{T} (r_t - \bar{r})^2)^2}
= \frac{\sigma_{\epsilon}^2}{\sum_{t=0}^{T} (r_t - \bar{r})^2}
$$

- Comme le dénominateur est une somme de termes positifs, donc lorsque $T \rightarrow \infty$, $\sum_{t=0}^{T} (r_t - \bar{r})^2 \rightarrow \infty$ et donc  $\lim_{T \rightarrow \infty} \mathrm{Var}(\hat{\beta})= 0$

## Question 4 : interprétation des résultats de l'estimateur des MCO

*A l'aide la fonction "lm" du logiciel R, estimer par les MCO les coeffi􏰧cients de l'équation 2. Commenter vos résultats en particulier le signe du coeffi􏰧cient et sa signi􏰥cativité. Qu'indiquent les statistiques de Student et de Fisher ainsi que le coeffi􏰧cient de détermination ?*\

```{r}
fit = lm(data$yield ~ data$rates)
summary(fit)
```

- $\widehat{\beta} = -0.0036653$ est de signe négatif, ce qui signifie que le earning yield diminue avec l'augmentation du taux sans risque.
- $\widehat{\alpha} = 0.0586936$ (l'`intercept`) est de signe positif, cela signifie que le earning yield vaut 0.0586936 lorsque le taux sans risque est nul.

- La statistique de Student (`t-value`) est un critère qui permet de juger sur l'hypothèse nulle suivante : H0 = {le coefficient considéré vaut 0}. On peut alors observer la p-value associée (`Pr(>|t|)`) : ici, sa valeur vaut `2.94e-08` pour $\widehat{\beta}$ et `< 2e-16` pour $\widehat{\alpha}$ et est donc inférieur aux seuils de significativité classiques (0.1%, 1%, 5% etc.), nous pouvons rejeter H0 et donc dire que ces coefficients sont significativement différents de 0.
  
- Le F-test considère l'hypothèse nulle suivante : H0 = {les coefficients (autre que l'`intercept`) valent tous 0}, il s'agit d'une comparaison avec le modèle constitué uniquement d'une estimation de l'`intercept` ie l'ordonnée à l'origine. La p-value associée à la statistique de Fisher valant `2.944e-08`, elle est inférieure aux seuils de c'est-à-dire qu'au moins une variable n'est pas significativement différente de significativité classiques (0.1%, 1%, 5% etc.) donc on peut rejeter cette hypothèse nulle, ie que le modèle dans l'ensemble est significatif.  On peut remarquer que comme il n'y a qu'une seule variable autre que l'ordonnée à l'origine, la p-valeur de la F-stat est la même que celle de la statistique de Student.

Le coefficient de détermination $R^2$ (`R-squared = 0.1229`) et le coefficient de détermination ajusté (`Adjusted R-squared = 0.1191`) sont tous les deux très faibles, indiquant que seulement une faible part d'environ 12% de la variable en question $\frac{E_t}{P_t}$ est expliquée par le modèle, donc la qualité de la régression est médiocre.

## Question 5 - Résidus estimés
*Réaliser un étude complète des résidus estimés : tracer leur densité, étudier leur normalité et vérifi􏰥er l'existence/l'absence d'autocorrélation et d'hétéscédasticité.*\


- Normalité des résidus du modèle : En visualisant la densité des résidus, nous voyons que celle-ci n'a pas la forme de cloche symétrique que possède une distribution normale classique. Cette idée est confirmée par le graphe quantile-quantile (qq-plot) avec : nous voyons que les points s'éloignent en queue de distribution. Le test de Shapiro-Wilk est effectué avec comme hypothèse nulle H0 = {l'échantillon est issu d'une population normalement distribué}, la p-valeur associée étant de ` 1.233e-07 < 0.05`, nous pouvons rejeter H0 : il est probable que les données ne soient pas issues d'une population normalement distribuée.

```{r}
plot(density(resid(fit)))
qqnorm(resid(fit))
qqline(resid(fit))

shapiro.test(resid(fit))
```

- Autocorrélation des résidus : En observant la fonction d'autocorrélation sur la série des résidus, on constate qu'il y a existence d'autocorrélation : la fonction d'autocorrélation prend des valeurs positives importantes (>0.2 en valeur absolue) pour des lags allant de 1 à 14 jours. Cela signifie que le modèle n'est pas optimal car les résidus peuvent être utilisés pour prédire les prochains résidus : il y a de l'information qui échappe au modèle actuel.


```{r}
plot(resid(fit), main="Evolution des résidus", xlab="date", ylab="résidu")
acf(resid(fit))
```

- Hétéroscédasticité des résidus : L'observation sur les résidus nous fait penser que ceux-ci sont hétéroscédastiques : la variance des résidus a augmenté puis diminué au cours du temps. Le test de Breusch-Pagan donne une p-valeur de `0.1577`, on peut rejeter l'hypothèse nulle H0 = {les données sont homoscédastiques} avec un seuil de confiance de 20%, ce qui est plutôt élevé comparé à tous les autres seuils de confiance obtenus précédemment.


```{r}
bptest(fit)
```

Conclusion : Outre le fait que le modèle ait un coefficient de détermination faible, l'étude des résidus a montré que ces derniers ne suivent pas une distribution normale, il y a présence d'autocorrélation et d'hétéroscédasticité, modéliser le earning yield en utilisant uniquement le taux sans risque ne permet pas d'avoir une modélisation où les hypothèses sous-jacente de l'esimateur des MCO ne sont respectées, il faudrait améliorer le modèle.

# Partie 2: Estimation d'une nouvelle spécifi􏰥cation et comparaison

## Question 6
*Calculer le taux d'intérêt réel. Estimer par la méthode des moindres carrés ordinaires cette nouvelle spéci􏰥cation. Améliore-t-elle le pouvoir explicatif du modèle? Justi􏰥er votre réponse.*\

Le taux d'intérêt réel est calculé par en utilisant l'indice des prix à la consommation (CPI) sur 12 mois de la façon suivante : 
$$\pi_t = 100 \frac{cpi_t - cpi_{t-12}}{cpi_{t-12}}$$
On omet donc les 12 premiers mois pour pour la construction du modèle Fed corrigé : 
$$\frac{E_t}{P_t} = \alpha + \beta (r_t - \pi_t) + \epsilon_t$$

```{r}
inflation = 100*(data$cpi[-(1:12)] / data$cpi[1:(length(data$cpi) - 12)] - 1)  # retrouver l'inflation à partir du CPI

data = data[-(1:12),]

data$inflation = inflation

data$real_rates = data$rates - data$inflation

fit2 = lm(data$yield ~ data$real_rates)

summary(fit2)

plot(data$real_rates, data$yield, xlab="Taux", ylab="Yield",
     main="Yield en fonction du taux")
abline(0.050445, -0.0062351, col="deeppink", lwd=2)
```

Concernant le pouvoir explicatif du modèle : 

- Le coefficient de détermination $R^2$ est de à présent 0.39, ce qui est une amélioration nette par rapport à précédemment où il était de 0.14.

- Toutefois, une analyse plus poussée des résidus montre que les résidus ne suivent toujours pas une distribution normale, ni décorrélés, ni homoscédastiques La fonction de densité présentent deux pics, les points en queue de distribution qui s'éloignent de la valeur théorique du diagramme quantile-quantile et le test de Shapiro-Wilk présente une p-valeur très faible de `0.004569` permettant de rejeter l'hypothèse nulle que les données suivent une distribution normale. La fonction d'autocorrélation montre que les résidus ne sont pas décorrélés. Le test de Breusch-Pagan donne une p-valeur de `0.7441` donc nous ne pouvons pas conclure sur l'homoscédasticité des résidus. 

L'amélioration du pouvoir explicatif du modèle se traduit alors uniquement par un meilleur coefficient de détermination, le modèle linéaire choisi ne permet pas d'obtenir des résidus conformes aux hypothèses de modélisation du modèle MCO.

```{r}
plot(resid(fit2), main="Evolution des residus", xlab="date", ylab="résidu")
plot(density(resid(fit2)))
qqnorm(resid(fit2))
qqline(resid(fit2))
shapiro.test(resid(fit2))
acf(resid(fit2))
bptest(fit2)
```


# Partie 3: Estimation d'une nouvelle spécifi􏰥cation : modèle  $ARMA(p,q)$


## Question 7

*Présenter le modèle ARMA et ses principales propriétés de manière précise et succinte.*\

Le modèle ARMA est un modèle mathématique visant à modéliser le comportement d'une série temporelle en fonction de ses valeurs historiques (c'est la partie autorégressive –AR– du modèle) mais aussi en fonction d'un bruit blanc qui viendrait perturber les données (d'où une moyenne mobile dans le modèle –MA–), dans le but de prédire les valeurs que prendrait la série temporelle dans le futur. \

Pour une variable représentée par une série temporelle, on introduit généralement les ordres p et q et on parle d'un modèle ARMA(p,q) pour modéliser une valeur au temps t à partir des p valeurs précédentes et de q bruits blancs indépendants et identiquement distribués : $$X_t = \phi_1X_{t-1} + ... + \phi_pX_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + ... + \theta_q \epsilon_{t-q} $$

## Question 8

*Identi􏰥er l'ordre du modèle ARMA à l'aide de deux méthodes ffdi􏰤érentes (on privilégiera un ap- proche parcimonieuse)*\

### Première approche

La première approche serait de regarder la fonction d'autocorrélation ainsi que la fonction d'autocorrélation partielle. En effet, celles-ci donnent des informations sur les ordres q et p respectivement du modèle que l'on pourrait utiliser pour modéliser la série temporelle des yields.


```{r}
acf(data$yield)  # fonction d'autocorrélation

pacf(data$yield)  # fonction d'autocorrélation partielle
```

La fonction d'autocorrélation sur le premier graphe ne nous apprend pas grand-chose sur la dépendance entre $X_t$ et d'éventuels bruits blancs car pratiquement tous les pics sont hors de la zone délimitée par les bandes bleues en pointillés. Les pics dans cette zone peuvent être considérés comme nuls. Or, on voit que les pics sont très au-delà de cette zone, et ceci pour des lags jusqu'à environ 19 : on ne peut donc pas présumer de la valeur de q. \


En revanche, à l'aide de la fonction d'autocorrélation partielle présentée sur le second graphe, on observe que deux pics ne se situent pas entre les pontillés bleus : ceux qui correspondent à un lag de 1 et à un lag de 2. On a donc deux pics qui se distringuent, et **on peut ainsi supposer que l'ordre pour la partie autorégressive du modèle, p, vaut 2**. \


On a considéré que le pic du lag à 12 était nul bien qu'il reflète une saisonnalité des données : la valeur du yield présente une certaine corrélation par rapport à sa valeur il y a 12 mois. Néanmoins, pour des raisons de simplicité, on ne traite pas cette corrélation. \


### Deuxième approche


La seconde approche serait une approche "force brute" : on teste toutes les combinaisons possibles de p et q (pour p et q entiers inférieurs à une valeur limite, typiquement 4), et on regarde quels modèles permettent de mieux modéliser les données empiriques, via la fonction ```arima``` par exemple. Pour cela, on s'attache à regarder des critères d'information tels que l'AIC (Akaike Information Criteria) ou le BIC (Bayesian Information Criteria) qui donnent une estimation de la qualité du modèle fitté. Ces critères font une sorte de compromis entre la complexité des modèles utilisés et la qualité d'estimation de ceux-ci par rapport aux données empiriques. On choisit ensuite le modèle qui donne le plus petit critère. \


La fonction ```arima``` ne renvoyant que l'AIC pour un modèle, on calcule le BIC à partir de la fonction ```BIC``` de R..

```{r, warning=FALSE}
order_max = 4

order_p = c()
order_q = c()
aic = c()
bic = c()

for(p in 0:order_max){
  for(q in 0:order_max){
    order_p = c(order_p, p)
    order_q = c(order_q, q)
    model = arima(data$yield, order=c(p, 0, q))
    
    aic = c(aic, model$aic)
    bic = c(bic, BIC(model))
  }
}

arma_models = data.frame(order_p, order_q, aic, bic)

arma_models[which(arma_models$aic == min(arma_models$aic)),]  # modèle avec AIC minimal
arma_models[which(arma_models$bic == min(arma_models$bic)),]  # modèle avec BIC minimal
```


Le modèle minimisant le critère AIC est un modèle ARMA(2,2) tandis que le modèle qui minimise le critère BIC est un modèle ARMA(2,0). On voit ainsi que les deux critères d'information ne sont pas équivalents : en effet, le critère BIC prend en compte la taille de l'échantillon. Quel que soit le critère à minimiser, le modèle correspondant donne un ordre p égal à 2, ce qui confirme la conclusion de la première approche. \


En fonction du critère à minimiser, on choisira plutôt un modèle ARMA(2, 0) ou un modèle ARMA(2, 2).

## Question 9

*Estimer le modèle identifié à l'aide de la fonction auto.arima et vérifier la qualité de votre estimation*\

```{r}
arima_data = auto.arima(data$yield, d=0, D=0, ic="aic")
mean(data$yield)

summary(arima_data)
```

```{r}
arima_data = auto.arima(data$yield, d=0, D=0, ic="bic")
mean(data$yield)

summary(arima_data)

```

Le moèle suggéré est un ARMA(2, 2), ce qui confirme la conclusion des deux approches de la question 8 concernant l'ordre p. Quant à l'ordre q, il semble qu'en accord avec la deuxième approche, ce soit l'AIC qui ait été privilégié par la fonction ```auto.arima```. \


La qualité d'estimation est plutôt bonne car elle donne une Root Mean Square Error (RMSE) de $0.002$, soit environ $0.002 / 0.045 \approx 4 \%$ de la moyenne des yields, de même pour la Mean Absolute Error (MAE) qui est de $0.001$, soit environ $0.001 / 0.045 \approx 2 \%$.


## Question 10

*Effectuer une prévision à l'aide de la commande predict sur horizon de trois périodes. Donner l'intervalle de confiance à 95%*\

```{r}
sigma2 = arima_data$sigma2

prediction = predict(arima_data, n.ahead=36)

prediction_upper = prediction$pred + 1.96*sigma2  # intervalle de confiance à 95%
prediction_lower = prediction$pred - 1.96*sigma2

```


On présente ci-dessous les prédictions ainsi que l'intervalle de confiance à 95% représenté par les droites en orange.

```{r}
plot(1:36, prediction$pred[1:36], xlab="Horizon de prédiction (mois)", ylab="Yield",
     main="Yield prédit en fonction de l'horizon de prédiction en mois",
     ylim=c(min(prediction_lower), max(prediction_upper)))
lines(1:36, prediction_lower, col="orange")
lines(1:36, prediction_upper, col="orange")
```


## Question 11

*Comparer  les deux derniers modèles estimés en utilisant ces deux critères (MAE et RMSE). Quel est celui qui affiche la meilleure performance ?*\

### MAE

```{r}
MAE_fit2 = mean(abs(fit2$residuals))
MAE_fit2
```

On trouve une MAE pour la régression linéaire (par rapport aux taux d'intérêt réels) de 0.008861187, elle est donc supérieure au 0.00127265 pour la MAE du modèle ARMA(2,2). Au sens de la MAE, c'est donc le modèle ARMA qui est meilleur.

### RMSE

```{r}
RMSE_fit2 = sqrt(mean(fit2$residuals^2))
RMSE_fit2
```

La RMSE pour la régression linéaire est de 0.01053751 alors que le modèle ARMA(2,2) donne une RMSE de 0.001906076. Le modèle ARMA est donc meilleur que la régression linéaire au sens de la RMSE. \


Ainsi, que ce soit au sens de la RMSE ou de la MAE, le modèle ARMA(2,2) semble être un meilleur modèle que la régression linéaire.


## Question 12

*A l'aide d'une routine que vous développez sour R, estimer les coefficients conformément à la méthode glissante. Tracer les courbes des coefficients $\beta_i$ estsimés ainsi que leur intervalle de confiance au seuile de 95%. Commenter*\

On prend une période de 100 mois sur laquelle on estime les $\beta_i$ : $\beta_i$ est donc le coefficient de $r_t$ dans l'équation de régression $\frac{E_t}{P_t} = \alpha + \beta r_t$ résolue à l'aide uniquement des données disponibles entre le mois $i$ et le mois $i + 99$. $\beta_1$ porte donc sur les données entre le mois $1$ et le mois $100$; $\beta_2$ sur celles entre le mois $2$ et le mois $101$ etc.

```{r}
period_move = 1
period_length = 100

nperiods = (length(data$price) - period_length) %/% period_move

beta = c()
lower = c()
upper = c()

for(i in 0:nperiods){
    
    yields = data$yield[(i*period_move+1):(i*period_move + period_length)]
    real_rates = data$real_rates[(i*period_move+1):(i*period_move + period_length)]
    fit_i = lm(yields ~ real_rates)
    confidence = confint(fit_i)
    
    beta = c(beta, fit_i$coefficients[2])
    lower = c(lower, confidence[2,1])
    upper = c(upper, confidence[2,2])
}

plot(0:nperiods + 1, beta, ylim=c(min(lower), max(upper)), xlab="Mois de début de période",
     ylab="Coefficient bêta",
     main="Coefficient bêta glissant")
lines(upper)
lines(lower)
```


 Globalement, le coefficient $\beta$ reste négatif et oscille autour d'une valeur moyenne à $-0.006$. On remarque toutefois que $\beta$ baisse notablement en valeur autour du 120e mois, c'est-à-dire si l'on essaie de régresser le yield par rapport aux taux sur des données postérieures à fin 2007. Cela peut s'expliquer par le fait que la crise de 2008 a notablement dégradé les taux qui sont devenus négatifs et moins volatils, d'où un dénominateur plus faible dans l'expression $$\hat{\beta} = \frac{Cov(\frac{E}{P}, r)}{\sigma_r^2}$$ ce qui pourrait expliquer pourquoi $\beta$ a augmenté en valeur absolue.
 
 
## Question 13

*Présenter le test CUSUM. Implémenter le test de CUSUM et commenter vos résultats *\

Le test CUSUM est un test fondé sur la somme cumulée (**cu**mulative **sum** en anglais) des résidus récursifs : au fur et à mesure que le modèle "grossit" avec un nombre croissant de données pour estimer le coefficient $\beta$, on en déduit les résidus récursifs qui représentent l'erreur successive entre les données observées et les données modélisées. \
La somme partielle est la statistique de test; elle permet de détecter tout changement structurel dans l'estimation : lorsque la somme sort d'un certain intervalle de stabilité, on décrète qu'il y a eu un changement significatif dans l'estimation, et qu'il y a donc instabilité du modèle. \


Lorsque l'on effectue un test CUSUM, l'hypothèse nulle est la constance des coefficients estimés par le modèle. Sous cette hypothèse nulle, il y a instabilité du modèle dès lors que la statistique de test sort de l'intervalle de stabilité.


```{r}
cusum_data = efp(data$yield ~ data$real_rates)
plot(cusum_data)

sctest = sctest(data$yield ~ data$real_rates)
sctest
```

On voit que la statistique de test n'est pas entièrement contenue dans l'intervalle de stabilité délimité par les droites en rouge. **Il y a donc instabilité du modèle**, ce qui est confirmé par la fonction ```sctest``` qui donne une p-valeur très basse, d'où un rejet de l'hypothèse nulle : le coefficient $\beta$ estimé n'est en fait pas constant sur toute la durée d'observation.